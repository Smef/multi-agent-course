{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_2/Quantization/TextStreamer_Meta_Llama_3_1_8B_Instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szSg-xijVEM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr_duA6kz4P9"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saQpB29nkwGl"
      },
      "source": [
        "## Using TextStreamer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBbu71zgnm4d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwn0ucrtnm4d"
      },
      "outputs": [],
      "source": [
        "def start_gpu_stat():\n",
        "    #@title Show current memory stats\n",
        "    #Set torch device to get properties global: torch.cuda.set_device(0)\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    initial_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "    return initial_gpu_memory, max_memory\n",
        "\n",
        "def final_gpu_stat(_initial_gpu_memory, _max_memory):\n",
        "    #@title Show final memory and time stats\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    used_memory_for_diff = round(used_memory - _initial_gpu_memory, 3)\n",
        "    used_percentage = round(used_memory         /_max_memory*100, 3)\n",
        "    diff_percentage = round(used_memory_for_diff/_max_memory*100, 3)\n",
        "\n",
        "    print(f\"Max memory = {_max_memory} GB.\")\n",
        "    print(f\"{_initial_gpu_memory} GB of INITIAL memory reserved.\")\n",
        "    print(f\"Peak reserved FINAL memory = {used_memory} GB.\")\n",
        "    print(f\"Peak reserved memory DIFFERENCE = {used_memory_for_diff} GB.\")\n",
        "    print(f\"Peak reserved memory % of FINAL memory = {used_percentage} %.\")\n",
        "    print(f\"Peak reserved memory % of DIFFERENCE memory = {diff_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wELsOde0nm4d"
      },
      "source": [
        "# Text Streaming Without Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHuonjNTiKPb"
      },
      "outputs": [],
      "source": [
        "model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct\" # Replace with your model\n",
        "\n",
        "# Load tokenizer and full precision model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "initial_gpu_memory, max_memory = start_gpu_stat()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "final_gpu_stat(initial_gpu_memory, max_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sZ5lDoL4yQ6"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "\n",
        "# Define Alpaca-style prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")  # instruction\n",
        "\n",
        "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)  # Move inputs to model's device\n",
        "\n",
        "# Initialize text streamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
        "\n",
        "# Generate response with streamer\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOKzkoe_nm4e"
      },
      "source": [
        "# Time To First Token (TTFT)\n",
        "Time required to process the prompt and then generate the first output token\n",
        "\n",
        "# Inter-token latency (ITL)\n",
        "Average time between consecutive tokens\n",
        "\n",
        "# End-to-end Latency  \n",
        "Total time taken to generate the entire response\n",
        "~ Average output length of tokens * Inter-token latency\n",
        "\n",
        "# Throughput\n",
        "Number of output tokens per second\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoOe8cFznm4e"
      },
      "outputs": [],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "# Define Alpaca-style prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")\n",
        "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Initialize variables for time measurements\n",
        "start_time = time.time()\n",
        "token_times = []\n",
        "\n",
        "# Initialize streamer\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "# Start generation in a separate thread\n",
        "thread = Thread(target=model.generate, kwargs={\n",
        "    'input_ids': inputs['input_ids'],\n",
        "    'attention_mask': inputs['attention_mask'],\n",
        "    'streamer': streamer,\n",
        "    'max_new_tokens': 100\n",
        "})\n",
        "thread.start()\n",
        "\n",
        "# Initialize a variable to store the model output\n",
        "model_output = \"\"\n",
        "first_token_time = None\n",
        "\n",
        "# Iterate over the streamer to get the generated text in chunks\n",
        "for i, new_text in enumerate(streamer):\n",
        "    model_output += new_text\n",
        "    print(new_text, end='')\n",
        "\n",
        "    # Measure time for the first token\n",
        "    if i == 0:\n",
        "        first_token_time = time.time()\n",
        "    # Measure time for each token\n",
        "    token_times.append(time.time())\n",
        "\n",
        "# Calculate end-to-end latency\n",
        "end_time = time.time()\n",
        "end_to_end_latency = end_time - start_time\n",
        "\n",
        "# Calculate time to first token\n",
        "ttft = first_token_time - start_time if first_token_time else 0\n",
        "\n",
        "# Calculate inter-token latency\n",
        "itl = sum(x - y for x, y in zip(token_times[1:], token_times[:-1])) / (len(token_times) - 1) if len(token_times) > 1 else 0\n",
        "\n",
        "# Calculate throughput\n",
        "throughput = len(tokenizer.encode(model_output)) / end_to_end_latency if model_output else 0\n",
        "\n",
        "print(\"\\nTime To First Token (TTFT):\", ttft)\n",
        "print(\"Inter-token latency (ITL):\", itl)\n",
        "print(\"End-to-end Latency:\", end_to_end_latency)\n",
        "print(\"Throughput:\", throughput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTHR-8MvvrY"
      },
      "source": [
        "# Text Streaming With Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89BBL2Donm4f"
      },
      "source": [
        "Shutdown and Restart the kernel before starting below cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPa8CI-mnm4f"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlXRyTWhnm4f"
      },
      "outputs": [],
      "source": [
        "def start_gpu_stat():\n",
        "    #@title Show current memory stats\n",
        "    #Set torch device to get properties global: torch.cuda.set_device(0)\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    initial_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "    return initial_gpu_memory, max_memory\n",
        "\n",
        "def final_gpu_stat(_initial_gpu_memory, _max_memory):\n",
        "    #@title Show final memory and time stats\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    used_memory_for_diff = round(used_memory - _initial_gpu_memory, 3)\n",
        "    used_percentage = round(used_memory         /_max_memory*100, 3)\n",
        "    diff_percentage = round(used_memory_for_diff/_max_memory*100, 3)\n",
        "\n",
        "    print(f\"Max memory = {_max_memory} GB.\")\n",
        "    print(f\"{_initial_gpu_memory} GB of INITIAL memory reserved.\")\n",
        "    print(f\"Peak reserved FINAL memory = {used_memory} GB.\")\n",
        "    print(f\"Peak reserved memory DIFFERENCE = {used_memory_for_diff} GB.\")\n",
        "    print(f\"Peak reserved memory % of FINAL memory = {used_percentage} %.\")\n",
        "    print(f\"Peak reserved memory % of DIFFERENCE memory = {diff_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V4lZeLtnm4f"
      },
      "outputs": [],
      "source": [
        "model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct\" # Replace with your model\n",
        "\n",
        "# 4-bit quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load tokenizer and model in 4-bit\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "initial_gpu_memory, max_memory = start_gpu_stat()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "final_gpu_stat(initial_gpu_memory, max_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohi4NSJTnm4f"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# Define Alpaca-style prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")  # instruction\n",
        "\n",
        "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)  # Move inputs to model's device\n",
        "\n",
        "# Initialize text streamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "# Generate response with streamer\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaRLZQ8rnm4f"
      },
      "outputs": [],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "# Define Alpaca-style prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")\n",
        "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Initialize variables for time measurements\n",
        "start_time = time.time()\n",
        "token_times = []\n",
        "\n",
        "# Initialize streamer\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "# Start generation in a separate thread\n",
        "thread = Thread(target=model.generate, kwargs={\n",
        "    'input_ids': inputs['input_ids'],\n",
        "    'attention_mask': inputs['attention_mask'],\n",
        "    'streamer': streamer,\n",
        "    'max_new_tokens': 100\n",
        "})\n",
        "thread.start()\n",
        "\n",
        "# Initialize a variable to store the model output\n",
        "model_output = \"\"\n",
        "first_token_time = None\n",
        "\n",
        "# Iterate over the streamer to get the generated text in chunks\n",
        "for i, new_text in enumerate(streamer):\n",
        "    model_output += new_text\n",
        "    print(new_text, end='')\n",
        "\n",
        "    # Measure time for the first token\n",
        "    if i == 0:\n",
        "        first_token_time = time.time()\n",
        "    # Measure time for each token\n",
        "    token_times.append(time.time())\n",
        "\n",
        "# Calculate end-to-end latency\n",
        "end_time = time.time()\n",
        "end_to_end_latency = end_time - start_time\n",
        "\n",
        "# Calculate time to first token\n",
        "ttft = first_token_time - start_time if first_token_time else 0\n",
        "\n",
        "# Calculate inter-token latency\n",
        "itl = sum(x - y for x, y in zip(token_times[1:], token_times[:-1])) / (len(token_times) - 1) if len(token_times) > 1 else 0\n",
        "\n",
        "# Calculate throughput\n",
        "throughput = len(tokenizer.encode(model_output)) / end_to_end_latency if model_output else 0\n",
        "\n",
        "print(\"\\nTime To First Token (TTFT):\", ttft)\n",
        "print(\"Inter-token latency (ITL):\", itl)\n",
        "print(\"End-to-end Latency:\", end_to_end_latency)\n",
        "print(\"Throughput:\", throughput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqt_UDsunm4f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}